# Embedding-LLM-finetuning-for-search

## Product Overview
Vector search is the preferred mechanism for getting to the intended content from a large text corpus where the search query is phrased more naturally. Large language models can embed the semantic meaning of the content into vectors and thus search systems can pull similar content through vector similarity computations. This solution helps customize the model that embeds the vector to understand the nature of questions and domain semantics. This alignment helps improve search results which is observed by improved rankings of relevant content. The input is a CSV with questions and corresponding answers - for example a FAQ dataset. The model is fine-tuned on this dataset. At the time of inference this trained model can be used to embed any text content. This means the trained model can be used to vectorize the final desired corpus and incoming queries for search.   


## Product Highlight
* The solution is intended to be used as part of a search and retrieval workflow. The fine-tuned model can be used to embed your documents to a vector database and vectorize incoming search queries.  
* The solution takes input in the form of simple Q&A pairs. This helps the LLM align with the type of questions that will be posed to the system in production. 
* Mphasis DeepInsights is a cloud-based cognitive computing platform that offers data extraction & predictive analytics capabilities. Need customized Machine Learning and Deep Learning solutions? Get in touch!

## Amazon Marketplace Link
The product can be found [here](https://aws.amazon.com/marketplace/pp/prodview-nsfhelo72jwrw)
